{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import wave\n",
    "import pyaudio\n",
    "import pyttsx3\n",
    "from pynput import keyboard\n",
    "from groq import Groq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Groq client (replace with your API key)\n",
    "GROQ_API_KEY = \"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\"\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Chat Groq model\n",
    "model = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY, model_name=\"llama-3.1-70b-versatile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TTS engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set the speech rate\n",
    "engine.setProperty('rate', 150)  # Adjust this value to make the voice slower or faster\n",
    "\n",
    "# Define recording parameters\n",
    "CHUNK = 1024  # number of audio frames per buffer\n",
    "FORMAT = pyaudio.paInt16  # each audio sample is stored as a 16-bit signed integer (2 bytes per sample)\n",
    "CHANNELS = 1  # number of audio channels to be recorded\n",
    "RATE = 44100  # how many audio samples are captured per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = False  # Track recording state\n",
    "frames = []  # To store audio frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for stop when press enter or esc\n",
    "def on_press(key):\n",
    "    \"\"\"\n",
    "    Callback function to handle key presses.\n",
    "    Stops the recording when Enter or Esc is pressed.\n",
    "    \"\"\"\n",
    "    global recording\n",
    "    if key == keyboard.Key.enter or key == keyboard.Key.esc:\n",
    "        recording = False\n",
    "        print(\"\\nRecording stopped.\")\n",
    "        return False  # Stop listening for key presses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio():\n",
    "    \"\"\"\n",
    "    Records audio and stops based on key press.\n",
    "    \"\"\"\n",
    "    global recording, frames\n",
    "    frames = []  # Reset frames before recording\n",
    "    recording = True  # Start recording\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording... Press 'Enter' or 'Esc' to stop.\")\n",
    "\n",
    "    while recording:\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the audio\n",
    "def save_audio():\n",
    "    \"\"\"\n",
    "    Saves the recorded audio to a file named 'audio.wav'.\n",
    "    \"\"\"\n",
    "    global frames  # Declare frames as global to modify it\n",
    "    filename = 'audio.wav'  # Use .wav extension since you're using the wave module\n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(pyaudio.PyAudio().get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    print(f\"Audio saved as: {filename}\")\n",
    "    frames = []  # Clear frames after saving\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion of audio to text and then text to audio\n",
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"\n",
    "    Transcribes the saved audio using Groq API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(audio_file, 'rb') as f:\n",
    "            transcription = client.audio.transcriptions.create(\n",
    "                file=f,\n",
    "                model=\"distil-whisper-large-v3-en\",\n",
    "                prompt=\"Specify context or spelling\",  # Optional\n",
    "                response_format=\"json\",  # Optional\n",
    "                language=\"en\",  # Optional\n",
    "                temperature=0.0  # Optional\n",
    "            )\n",
    "\n",
    "        print(f\"Transcription: {transcription.text}\")\n",
    "        return transcription.text\n",
    "\n",
    "    except Groq.APIError as e:\n",
    "        print(f\"Groq API Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"\n",
    "Answer the question below.Don't add * in the headings of the output.\n",
    "\n",
    "Here is the conversation history:{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# chain the model and prompt\n",
    "chain = prompt | model\n",
    "\n",
    "# handle conversation and model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_model(user_input, context=\"\"):\n",
    "    \"\"\"\n",
    "    Pass the user input to the LLM and get the response.\n",
    "    \"\"\"\n",
    "    result = chain.invoke({\"context\": context, \"question\": user_input})\n",
    "    print(\"Bot:\", result.content)\n",
    "\n",
    "    # Append the conversation to context\n",
    "    new_context = f\"{context}\\nUser: {user_input}\\nAI: {result.content}\"\n",
    "\n",
    "    return result.content, new_context\n",
    "\n",
    "# Main function to handle everything: voice, model interaction, and text-to-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to handle everything: voice, model interaction, and text-to-speech\n",
    "def handle_conversation():\n",
    "    context = \"\"\n",
    "    print(\"Welcome to the AI chatbot. How may I help you?\")\n",
    "\n",
    "    while True:\n",
    "        # Listen for key presses to stop recording\n",
    "        listener = keyboard.Listener(on_press=on_press)\n",
    "        listener.start()\n",
    "\n",
    "        # Start recording audio\n",
    "        record_audio()\n",
    "\n",
    "        # Wait for listener to finish (when recording stops)\n",
    "        listener.join()\n",
    "\n",
    "        # Save and process the recorded audio\n",
    "        saved_audio = save_audio()\n",
    "        user_input = transcribe_audio(saved_audio)\n",
    "\n",
    "        if user_input.strip():  # If transcription is not empty\n",
    "            # Get response from the model\n",
    "            response, context = process_with_model(user_input, context)\n",
    "\n",
    "            # Speak the response\n",
    "            engine.say(response)\n",
    "            engine.runAndWait()\n",
    "\n",
    "        # Optional: Clean up the saved file\n",
    "        # os.remove(saved_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI chatbot. How may I help you?\n",
      "Recording... Press 'Enter' or 'Esc' to stop.\n",
      "\n",
      "Recording stopped.\n",
      "Audio saved as: audio.wav\n",
      "Transcription:  Sabins law. Yellow lady to all of D. Hey.\n",
      "Bot: Sabins law is not provided in the conversation history. However, I can provide information on Sabin's law. \n",
      "\n",
      "Sabin's law is related to the field of ophthalmology. It states that the direction of the visual field defect is related to the location of the lesion in the visual pathway.\n",
      "Recording... Press 'Enter' or 'Esc' to stop.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#if __name__ == \"__main__\":\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mhandle_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mhandle_conversation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m listener\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Start recording audio\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mrecord_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Wait for listener to finish (when recording stops)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m listener\u001b[38;5;241m.\u001b[39mjoin()\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mrecord_audio\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording... Press \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEsc\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to stop.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m recording:\n\u001b[1;32m---> 20\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m     23\u001b[0m stream\u001b[38;5;241m.\u001b[39mstop_stream()\n",
      "File \u001b[1;32md:\\ITSOLERA\\New\\Backend\\venv\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recording stopped.\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "handle_conversation() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
